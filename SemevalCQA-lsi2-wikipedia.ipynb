{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "\n",
    "wiki = WikiCorpus('../corpus/arwiki-latest-pages-articles.xml.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "corpus = [{'id': 'doc_%i' % num, 'tokens': text}\n",
    "    for num, text in enumerate(wiki.get_texts())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PyArabic import ArabicPreprocessor\n",
    "\n",
    "preprocessor = ArabicPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "93307e47-1ed3-4c84-9091-fb61c4b0cbcf"
    }
   },
   "outputs": [],
   "source": [
    "class LabeledQID(object):\n",
    "    def __init__(self, filename, qid):\n",
    "        self.filename = filename\n",
    "        self.qid = qid\n",
    "    def __iter__(self):\n",
    "        \n",
    "        # Loading test set\n",
    "        tree = etree.parse(self.filename)\n",
    "        \n",
    "        # {QID, Qtext} dictionary for questions\n",
    "        questions = {}\n",
    "\n",
    " \n",
    "        sentence = tree.xpath('Question[@QID=' + self.qid + ']/Qtext')[0].text\n",
    "        uid = 0\n",
    "        sentence = preprocessor.removeStopwords(sentence)\n",
    "        tokens = preprocessor.tokenize(sentence)\n",
    "        tokens = map(preprocessor.deNoise, tokens)\n",
    "        devocalize_tokens = map(preprocessor.removeDiacritics, tokens)\n",
    "        denoised_tokens = map(preprocessor.deNoise, devocalize_tokens)\n",
    "        normalized_tokens = map(preprocessor.normalizeAlef, denoised_tokens)\n",
    "        normalized_tokens = map(preprocessor.normalizeAggressive, normalized_tokens)\n",
    "        lemmatized_tokens = map(preprocessor.lemmatize, normalized_tokens)\n",
    "\n",
    "        yield LabeledSentence(words=[w for w in tokens], tags=['%s' % uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8888ea1b-de0b-4876-9133-77ac55f8e080"
    }
   },
   "outputs": [],
   "source": [
    "class LabeledQAPair(object):\n",
    "    def __init__(self, filename, qid):\n",
    "        self.filename = filename\n",
    "        self.qid = qid\n",
    "    def __iter__(self):\n",
    "        \n",
    "        # Loading test set\n",
    "        tree = etree.parse(self.filename)\n",
    "        \n",
    "        # {QID, Qtext} dictionary for questions\n",
    "        questions = {}\n",
    "\n",
    "        for qapair in tree.xpath('Question[@QID=' + self.qid + ']/QApair'):\n",
    "            qaid = qapair.get('QAID')\n",
    "            qaquestion = qapair.xpath('QAquestion')[0].text\n",
    "            qaanswer = qapair.xpath('QAanswer')[0].text\n",
    "           \n",
    "            qaquestion = preprocessor.removeStopwords(qaquestion)\n",
    "            tokens = preprocessor.tokenize(qaquestion)\n",
    "            tokens = map(preprocessor.deNoise, tokens)\n",
    "            devocalize_tokens = map(preprocessor.removeDiacritics, tokens)\n",
    "            denoised_tokens = map(preprocessor.deNoise, devocalize_tokens)\n",
    "            normalized_tokens = map(preprocessor.normalizeAlef, denoised_tokens)\n",
    "            normalized_tokens = map(preprocessor.normalizeAggressive, normalized_tokens)\n",
    "            lemmatized_tokens = map(preprocessor.lemmatize, normalized_tokens)\n",
    "        \n",
    "            yield LabeledSentence(words=[w for w in tokens], tags=['%s' % qaid])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4cc94538-b881-40e4-a287-b330346f57f3"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yassine\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Yassine\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from lxml import etree\n",
    "from collections import OrderedDict\n",
    "\n",
    "class LabeledQuestion(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def __iter__(self):\n",
    "        \n",
    "        # Loading test set\n",
    "        tree = etree.parse(self.filename)\n",
    "        \n",
    "        # {QID, Qtext} dictionary for questions\n",
    "        questions = {}\n",
    "\n",
    "        # {QID, [(QAquestion, QAanswer)]} dictionary for question/answer pairs\n",
    "        pairs = {}\n",
    "        qid_qaid = {}\n",
    "        for question in tree.xpath('Question'):\n",
    "            # construct questions dictionary\n",
    "            qid = question.get('QID')\n",
    "            qtext = question.xpath('Qtext')[0].text\n",
    "            \n",
    "            qtext = preprocessor.removeStopwords(qtext)\n",
    "            tokens = preprocessor.tokenize(qtext)\n",
    "            tokens = map(preprocessor.deNoise, tokens)\n",
    "            devocalize_tokens = map(preprocessor.removeDiacritics, tokens)\n",
    "            denoised_tokens = map(preprocessor.deNoise, devocalize_tokens)\n",
    "            normalized_tokens = map(preprocessor.normalizeAlef, denoised_tokens)\n",
    "            normalized_tokens = map(preprocessor.normalizeAggressive, normalized_tokens)\n",
    "            lemmatized_tokens = map(preprocessor.lemmatize, normalized_tokens)\n",
    "            \n",
    "            yield LabeledSentence(words=[w for w in tokens], tags=['%s' % qid])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from simserver import SessionServer\n",
    "\n",
    "service = SessionServer('../tmp/')\n",
    "\n",
    "service.train(corpus, method='lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "754ca4be-c68b-4a32-b027-92313a267937"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class QuestionPairSimilarity(object):\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        qs = LabeledQuestion('../input/SemEval2016-Task3-CQA-MD-test.xml')\n",
    "        for q in qs:\n",
    "            \n",
    "            \n",
    "            service.drop_index()\n",
    "            qid = q.tags[0]\n",
    "            print qid\n",
    "            questions = LabeledQID('../input/SemEval2016-Task3-CQA-MD-test.xml', qid)\n",
    "            pairs = LabeledQAPair('../input/SemEval2016-Task3-CQA-MD-test.xml', qid)\n",
    "\n",
    "            for question in questions:\n",
    "                pass\n",
    "\n",
    "            query = [w for w in question.words]\n",
    "            \n",
    "            question_document = {}\n",
    "            question_document['id'] = qid\n",
    "            question_document['tokens'] = query\n",
    "            \n",
    "            #msg = repr([x.encode(sys.stdout.encoding) for x in query]).decode('string-escape')\n",
    "            question_documents = []\n",
    "            question_documents.append(question_document)\n",
    "            service.index(question_documents)\n",
    "            \n",
    "            for index, pair in enumerate(pairs):\n",
    "                \n",
    "                qaid = pair.tags\n",
    "                document = [w for w in pair.words]\n",
    "                \n",
    "                pair_document = {}\n",
    "                pair_document['id'] = qaid[0]\n",
    "                pair_document['tokens'] = document\n",
    "                \n",
    "                pair_documents = []\n",
    "                pair_documents.append(pair_document)\n",
    "\n",
    "                service.index(pair_documents)\n",
    "                \n",
    "            similarities = service.find_similar(qid)\n",
    "                #msg = repr([x.encode(sys.stdout.encoding) for x in document]).decode('string-escape')\n",
    "                #if len(query) > 0 and len(document) > 0:\n",
    "                #     score = (model.n_similarity([w for w in query], [w for w in document]))\n",
    "                #else:\n",
    "                #    score = 0.0\n",
    "            for qaid, score, _ in similarities:\n",
    "                yield qid, qaid, score\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5e8d057a-ad17-4331-8327-04f7baa75e19"
    }
   },
   "outputs": [],
   "source": [
    "scored_questions = QuestionPairSimilarity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2943cf66-b61a-4b60-bf43-709972891c8b"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "25b4f788-4c26-4864-a4b7-2d443ad7219c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201399\n",
      "200902\n",
      "200172\n",
      "200875\n",
      "200030\n",
      "200066\n",
      "201135\n",
      "201425\n",
      "201499\n",
      "200360\n",
      "201430\n",
      "200241\n",
      "200477\n",
      "200369\n",
      "201475\n",
      "200783\n",
      "200211\n",
      "200988\n",
      "200323\n",
      "201158\n",
      "201214\n",
      "201272\n",
      "200975\n",
      "201583\n",
      "200484\n",
      "201329\n",
      "200721\n",
      "200400\n",
      "200763\n",
      "201308\n",
      "200553\n",
      "201167\n",
      "201193\n",
      "201564\n",
      "201317\n",
      "200817\n",
      "200209\n",
      "201521\n",
      "201179\n",
      "201571\n",
      "200797\n",
      "200754\n",
      "200852\n",
      "201556\n",
      "200620\n",
      "200859\n",
      "200403\n",
      "200522\n",
      "201057\n",
      "201294\n",
      "200581\n",
      "200789\n",
      "201171\n",
      "200904\n",
      "201008\n",
      "201455\n",
      "200494\n",
      "200521\n",
      "200280\n",
      "201244\n",
      "201516\n",
      "200085\n",
      "200805\n",
      "201177\n",
      "201405\n",
      "200726\n",
      "201377\n",
      "201285\n",
      "200523\n",
      "201572\n",
      "200724\n",
      "200938\n",
      "200374\n",
      "200355\n",
      "200515\n",
      "201446\n",
      "200509\n",
      "201436\n",
      "200202\n",
      "201270\n",
      "200719\n",
      "201554\n",
      "201254\n",
      "200775\n",
      "200750\n",
      "201274\n",
      "200058\n",
      "201229\n",
      "200827\n",
      "201321\n",
      "200188\n",
      "201288\n",
      "201116\n",
      "200885\n",
      "201265\n",
      "201333\n",
      "200767\n",
      "201451\n",
      "201342\n",
      "201258\n",
      "201492\n",
      "200174\n",
      "201157\n",
      "201216\n",
      "200572\n",
      "200841\n",
      "201543\n",
      "201458\n",
      "200462\n",
      "201130\n",
      "201287\n",
      "201074\n",
      "200420\n",
      "200480\n",
      "201203\n",
      "201303\n",
      "201391\n",
      "201104\n",
      "200912\n",
      "200723\n",
      "200979\n",
      "200993\n",
      "200144\n",
      "201175\n",
      "200152\n",
      "201551\n",
      "200969\n",
      "201535\n",
      "201566\n",
      "201324\n",
      "200786\n",
      "201152\n",
      "200185\n",
      "200184\n",
      "200728\n",
      "200926\n",
      "200079\n",
      "201476\n",
      "200407\n",
      "200607\n",
      "201246\n",
      "200048\n",
      "200486\n",
      "201509\n",
      "201086\n",
      "200415\n",
      "201301\n",
      "201525\n",
      "201412\n",
      "200070\n",
      "201488\n",
      "201243\n",
      "200271\n",
      "200584\n",
      "201053\n",
      "200575\n",
      "201549\n",
      "201168\n",
      "201033\n",
      "201247\n",
      "200487\n",
      "200446\n",
      "200764\n",
      "200914\n",
      "200629\n",
      "201388\n",
      "201021\n",
      "201579\n",
      "201105\n",
      "200549\n",
      "201050\n",
      "201422\n",
      "201413\n",
      "200354\n",
      "201354\n",
      "201505\n",
      "200970\n",
      "201269\n",
      "200843\n",
      "201432\n",
      "201438\n",
      "201375\n",
      "200643\n",
      "201097\n",
      "201271\n",
      "200628\n",
      "200156\n",
      "200455\n",
      "200623\n",
      "201517\n",
      "200457\n",
      "200335\n",
      "201574\n",
      "200285\n",
      "200794\n",
      "201085\n",
      "200464\n",
      "200619\n",
      "201302\n",
      "200686\n",
      "201386\n",
      "201131\n",
      "201233\n",
      "200368\n",
      "201127\n",
      "201501\n",
      "201524\n",
      "200412\n",
      "200528\n",
      "200402\n",
      "201283\n",
      "200948\n",
      "200100\n",
      "200564\n",
      "200471\n",
      "200907\n",
      "200116\n",
      "201367\n",
      "200140\n",
      "201102\n",
      "200055\n",
      "201546\n",
      "201188\n",
      "200286\n",
      "201310\n",
      "200655\n",
      "201093\n",
      "201338\n",
      "200991\n",
      "201264\n",
      "200980\n",
      "201189\n",
      "201110\n",
      "200792\n",
      "200995\n",
      "201332\n",
      "200434\n",
      "200711\n",
      "200702\n",
      "200961\n",
      "200974\n",
      "201299\n",
      "201548\n",
      "200391\n",
      "200814\n",
      "201389\n",
      "201573\n",
      "200759\n",
      "200238\n",
      "201129\n"
     ]
    }
   ],
   "source": [
    "scores = defaultdict(list)\n",
    "for qid, qaid, score in scored_questions:\n",
    "    scores[qid].append({'qaid': qaid, 'score':score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "48ea27c6-336f-4783-9cd6-ac9470539f73"
    }
   },
   "outputs": [],
   "source": [
    "percentiles = defaultdict(list)\n",
    "data = []\n",
    "myscores = defaultdict(lambda : defaultdict(int))\n",
    "for qid in scores.keys():\n",
    "    for dic in scores[qid]:\n",
    "        qaid = dic['qaid']\n",
    "        score = dic['score']\n",
    "        data.append(score)\n",
    "        myscores[qid][qaid] = dic['score']\n",
    "    percentiles[qid].append(np.percentile(data, 75))\n",
    "    percentiles[qid].append(np.percentile(data, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f77cb3d1-97b0-452f-80c2-8dad129f4640"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from lxml import etree\n",
    "#tree = etree.parse('SemEval2016-Task3-CQA-MD-test-input-Arabic.xml')\n",
    "\n",
    "with open('../output/SemEval2016-Task3-CQA-MD-qa-subtaskD.xml.pred', 'w') as f:\n",
    "    questions = LabeledQuestion('../input/SemEval2016-Task3-CQA-MD-test.xml')\n",
    "    for question in questions:\n",
    "        qid = question.tags[0]\n",
    "        pairs = LabeledQAPair('../input/SemEval2016-Task3-CQA-MD-test.xml', qid)\n",
    "        for pair in pairs:\n",
    "            qaid = pair.tags[0]\n",
    "            if qid <> qaid:\n",
    "                relevance = 'false'\n",
    "                if qid in myscores.keys():\n",
    "                    score = myscores[qid][qaid]\n",
    "                    if score > percentiles[qid][0]:\n",
    "                        relevance = 'true'\n",
    "                else:\n",
    "                    score = 0\n",
    "                f.write('%s\\t%s\\t0\\t%f\\t%s\\n' % (qid, qaid, score, relevance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "aa3a30e8-1ab5-4159-9165-891596376ab1",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
